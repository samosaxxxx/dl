1 - ADAGRAD OPTIMIZER
import numpy as np
# Define the function and its gradients
def f(x, y):
    return x**2 + y**2
def grad_f(x, y):
    return np.array([2 * x, 2 * y])
# Parameters
eta = 0.1  # Learning rate
epsilon = 1e-8  # Convergence threshold
x = -0.1659  # Initial x
y = 0.4406   # Initial y
bounds = [-1, 1]  # Bounds for x and y
# Initialize sum of squared gradients
sum_sq_grad = np.array([0.0, 0.0])
# Iteration counter
iteration = 0
max_iterations = 10000  # Safety limit
while True:
    # Compute gradients
    grad = grad_f(x, y)
    # Update sum of squared gradients
    sum_sq_grad += grad**2
    # Compute adaptive learning rate
    adaptive_lr = eta / (np.sqrt(sum_sq_grad) + epsilon)
    # Update parameters
    x -= adaptive_lr[0] * grad[0]
    y -= adaptive_lr[1] * grad[1]
    # Enforce bounds
    x = np.clip(x, bounds[0], bounds[1])
    y = np.clip(y, bounds[0], bounds[1])
    # Increment iteration
    iteration += 1
    # Print progress at each iteration
    print(f"Iteration {iteration}: x = {x:.8f}, y = {y:.8f}, f(x, y) = {f(x, y):.8f}")
    # Check convergence
    if np.linalg.norm(grad) < epsilon or iteration >= max_iterations:
        break
# Output results
print(f"\nFinal x: {x:.8f}, y: {y:.8f}")
print(f"Function value: {f(x, y):.8f}")
print(f"Total Iterations: {iteration}")

2 - MOMENTUM GRADIENT
import numpy as np
# The objective function
def objective(x, y):
    return x**2.0 + y**2.0
# The derivative of the objective function
def derivative(x, y):
    return np.array([x * 2.0, y * 2.0])
# Gradient descent algorithm with momentum
def momentum_gradient_descent(objective, derivative, bounds, n_iter, learning_rate, momentum):
    # Generate an initial point
    solution = bounds[:, 0] + np.random.rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])
    # Initialize the change vector
    change = 0.0
    # Run the gradient descent
    for i in range(n_iter):
        # Calculate the gradient
        gradient = derivative(solution[0], solution[1])
        # Calculate the new change vector
        new_change = learning_rate * gradient + momentum * change
        # Update the solution
        solution = solution - new_change
        # Store the new change
        change = new_change
        # Evaluate the candidate point
        solution_eval = objective(solution[0], solution[1])
        # Report progress
        print('>%d f(%s) = %.5f' % (i, solution, solution_eval))
    return [solution, solution_eval]
# Seed the pseudo-random number generator
np.random.seed(1)
# Define the range for input
bounds = np.asarray([[-1.0, 1.0], [-1.0, 1.0]])
# Define the total iterations
n_iter = 30
# Define the learning rate
learning_rate = 0.1
# Define the momentum
momentum = 0.3
# Perform the gradient descent search with momentum
best, score = momentum_gradient_descent(objective, derivative, bounds, n_iter, learning_rate, momentum)
print('Done!')
print('f(%s) = %f' % (best, score))

3 - ADAM OPTIMIZER
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import numpy as np
# Generate some sample data
X = np.array([-1, 0, 1, 2, 3, 4], dtype=float)
Y = np.array([-3, -1, 1, 3, 5, 7], dtype=float)
# Create a simple sequential model
model = Sequential()
model.add(Dense(1, input_dim=1))
# Compile the model with the Adam optimizer
# The learning rate can be tuned
adam = Adam(learning_rate=0.1)
model.compile(loss='mean_squared_error', optimizer=adam)
# Train the model
model.fit(X, Y, epochs=100, verbose=0)
# Make predictions
predictions = model.predict(X)
print("Predictions:", predictions.flatten())
# Print the learned weights
print("Learned weights (w, b):", [layer.get_weights() for layer in model.layers])

4 - RMSPROP(Root Mean Square Propogation)
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import RMSprop
import numpy as np
# Generate some sample data
X = np.array([-1, 0, 1, 2, 3, 4], dtype=float)
Y = np.array([-3, -1, 1, 3, 5, 7], dtype=float)
# Create a simple sequential model
model = Sequential()
model.add(Dense(1, input_dim=1))
# Compile the model with the RMSprop optimizer
# The learning rate can be tuned
rmsprop = RMSprop(learning_rate=0.1)
model.compile(loss='mean_squared_error', optimizer=rmsprop)
# Train the model
model.fit(X, Y, epochs=100, verbose=0)
# Make predictions
predictions = model.predict(X)
print("Predictions:", predictions.flatten())
# Print the learned weights
print("Learned weights (w, b):", [layer.get_weights() for layer in model.layers])

5 - STOCHASTIC GRADIENT DESCENT
import numpy as np
# Define the function and its gradients
def f(x, y):
    return x**2 + y**2
def grad_f(x, y):
    return np.array([2 * x, 2 * y])
# Parameters
eta = 0.1  # Learning rate
epsilon = 1e-8  # Convergence threshold
x = -0.1659  # Initial x
y = 0.4406   # Initial y
bounds = [-1, 1]  # Bounds for x and y
# Iteration counter
iteration = 0
max_iterations = 10000  # Safety limit
while True:
    # Compute gradients
    grad = grad_f(x, y)
    # Update parameters (SGD update)
    x -= eta * grad[0]
    y -= eta * grad[1]
    # Enforce bounds
    x = np.clip(x, bounds[0], bounds[1])
    y = np.clip(y, bounds[0], bounds[1])
    # Increment iteration
    iteration += 1
    # Print progress at each iteration
    print(f"Iteration {iteration}: x = {x:.8f}, y = {y:.8f}, f(x, y) = {f(x, y):.8f}")
    # Check convergence
    if np.linalg.norm(grad) < epsilon or iteration >= max_iterations:
        break
# Output results
print(f"\nFinal x: {x:.8f}, y: {y:.8f}")
print(f"Function value: {f(x, y):.8f}")
print(f"Total Iterations: {iteration}")
